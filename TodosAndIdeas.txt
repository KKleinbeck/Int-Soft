Properly benchmark recursive attention algorithm
  Precisely time how much time is spend in each subcall.


Memory efficient DataLoader:
  Flux's current data loader allocates space of the _same size as the training data_ upon
  calling it (its iterator). Obviously, this implies we loose ~ 50% of GPU memory, which is not
  acceptable. Write custom data loader which does not allocates that much memory.

  Even with this out of the way it is obvious that memory will get sparse for large data sets.
  For this I propose a two Stage DataLoader:
  - Stage 1: RAM -> GPU
    Store samples in the RAM and only load chunks into the GPU. When one chunk is used, free the
    memory and get the next chunk from RAM.

  - Stage 2: HDD -> RAM
    Eventually RAM will be sparse as well. In the background, perform loading of new samples
    whenever space is freed up.


Dropout
  Read into Dropout (what are good values; how to adept it during training). This can probably
  benefit training a lot.


Interface for a maths library
  Start thinking about a proper interface for a maths library. How should it look like? How should
  the user be able to interact with it? What features should initially be included? Which of those
  are from exact algorithms, which are learned algorithms?

Implement a tree based data generation
  This would probably benefits an eventual math library and give new opportunities for optimization
  and simplification of data generation.
